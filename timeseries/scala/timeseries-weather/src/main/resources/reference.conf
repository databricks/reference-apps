####################################
# Reference Config File #
####################################
include "application"

spark {
  # The Spark master host. Set first by environment if exists. Then system, then config.
  # Options: spark://host1:port1,host2:port2
  # - "local" to run locally with one thread,
  # - local[4]" to run locally with 4 cores
  # - the master IP/hostname to run on a Spark standalone cluster
  # - if not set, defaults to "local[*]" to run with enough threads
  # Supports optional HA failover for more than one: host1,host2..
  # which is used to inform: spark://host1:port1,host2:port2
  master = ${?SPARK_HA_MASTER}
  cleaner.ttl = ${?SPARK_CLEANER_TTL}

  # The batch interval must be set based on the latency requirements
  # of your application and available cluster resources.
  streaming.batch.interval = ${?SPARK_STREAMING_BATCH_INTERVAL}
}

# Undefined values are expected to be set by the user in their application.conf file
# Defined values are defaults which can be overridden by the user in their application.conf file
# We first check for a setting in the environment deployed to, then whether a java system property
# exists to use, and finally the configured one. The users configured values would override
# any configured setting in this file.
# For environment keys, set the key name noted, for instance: ${?CASSANDRA_KEYSPACE}, your
# environment should be CASSANDRA_KEYSPACE="my_keyspace" or CASSANDRA_RPC_PORT=9161
cassandra {

  # The contact point to connect to the Cassandra cluster.
  # Accepts a comma-separated string of hosts. Override with -Dspark.cassandra.connection.host.
  connection.host = ${?CASSANDRA_SEEDS}

  # Cassandra thrift port. Defaults to 9160. Override with -Dspark.cassandra.connection.rpc.port.
  connection.rpc.port = ${?CASSANDRA_RPC_PORT}

  # Cassandra native port. Defaults to 9042. Override with -Dspark.cassandra.connection.native.port.
  connection.native.port = ${?CASSANDRA_NATIVE_PORT}

  # Auth: These are expected to be set in the env by chef, etc.
  # The username for authentication. Override with -Dspark.cassandra.auth.username.
  auth.username = ${?CASSANDRA_AUTH_USERNAME}
  # The password for authentication. Override with -Dspark.cassandra.auth.password.
  auth.password = ${?CASSANDRA_AUTH_PASSWORD}

}

akka {

  loglevel = "DEBUG"
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  log-dead-letters = off
  log-dead-letters-during-shutdown = off

  remote {
    log-remote-lifecycle-events = off
    netty.tcp.port = 2550
  }

  actor {
    provider = "akka.cluster.ClusterActorRefProvider"
    default-dispatcher {
      # Throughput for default Dispatcher, set to 1 for as fair as possible
      throughput = 10
    }
  }

  cluster {
    seed-nodes = []
    auto-down-unreachable-after = 5s
    gossip-interval = 1s
    log-info = on
    metrics {
      collect-interval = 5s
      gossip-interval = 5s
    }
  }
}

kafka {
  hosts = [${?KAFKA_HOSTS}]
  ingest-rate = 1s

  group.id = "killrweather.group"
  topic.raw = "killrweather.raw"
  partitioner.fqcn = "kafka.producer.DefaultPartitioner"
  encoder.fqcn = "kafka.serializer.StringEncoder"
  decoder.fqcn = "kafka.serializer.StringDecoder"
  batch.send.size = 10
}
